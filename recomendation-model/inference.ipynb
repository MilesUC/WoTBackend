{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sagemaker.tensorflow import TensorFlowPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar recursos de NLTK necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"231205-012632-recomendation-model\" # Cambia cuando se reentrena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TensorFlowPredictor(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = '26908e63-8d63-4227-bc73-94e596e107c0' # Id tomado al azar\n",
    "communityId = np.array(['10']) # Id tomado al azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(userId):\n",
    "    db = pymysql.connect(host='wot-database.cvaeffy0qj7k.us-east-1.rds.amazonaws.com', user='wotAdmin', password='U35GebgcH7qD', db='develop')\n",
    "    cursor = db.cursor()\n",
    "\n",
    "    # Set users dataframe\n",
    "    # Get column names\n",
    "    cursor.execute(\"show columns from usuarias;\")\n",
    "    users_column_names = cursor.fetchall()\n",
    "    users_column_names = [column[0] for column in users_column_names]\n",
    "\n",
    "    # Get users data\n",
    "    cursor.execute(f\"SELECT * FROM usuarias WHERE id = '{userId}';\")\n",
    "    output = cursor.fetchall()\n",
    "    users_df = pd.DataFrame(output, columns=users_column_names)\n",
    "    users_df = users_df.set_index('id')\n",
    "\n",
    "    # Normalize data\n",
    "    users_df.drop([\n",
    "        'rut',\n",
    "        'nombre',\n",
    "        'apellido',\n",
    "        'celular',\n",
    "        'mail',\n",
    "        'empresa_actual',\n",
    "        'empresa_adicional',\n",
    "        'id_cargo',\n",
    "        'id_cargo_adicional',\n",
    "        'id_anios_experiencia',\n",
    "        'experienciaDirectorios',\n",
    "        'altaDireccion',\n",
    "        'redesSociales',\n",
    "        'factor', # Podría ser relevante, pero se tiene muy poca data de este campo\n",
    "        'nombrePuebloOriginario', # Podría ser relevante, pero se tiene muy poca data de este campo\n",
    "        'id_region_con_compromiso',\n",
    "        'region_domicilio',\n",
    "        'id_posibilidad_cambiarse_region',\n",
    "        'disposicion_viajar',\n",
    "        'id_modalidad',\n",
    "        'id_jornada',\n",
    "        'id_conocio_wot',\n",
    "        'id_rol',\n",
    "        'declaracion',\n",
    "        'id_pais_domicilio', # Podría ser útil si se hacen comunidades enfocadas en países, pero por mientras no se considerará\n",
    "        'universidad', # Podría ser útil si se hacen comunidades enfocadas en universidades, pero por mientras no se considerará\n",
    "        'created_at',\n",
    "        'updated_at'\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "    # Se obtienen los valores correspondientes a los ids\n",
    "    cursor.execute(\"SELECT id, nombre_industria FROM industrias;\")\n",
    "    output = cursor.fetchall()\n",
    "    industries_df = pd.DataFrame(output, columns=['id', 'nombre'])\n",
    "    industries_df.set_index('id', inplace=True)\n",
    "\n",
    "    cursor.execute(\"SELECT id, personalidad FROM formulario_personalidades;\")\n",
    "    output = cursor.fetchall()\n",
    "    personalities_df = pd.DataFrame(output, columns=['id', 'personalidad'])\n",
    "    personalities_df.set_index('id', inplace=True)\n",
    "\n",
    "    # Se reemplazan los ids por los valores correspondientes\n",
    "    users_df['id_industria_actual'].replace(industries_df['nombre'], inplace=True)\n",
    "    users_df['id_industria_adicional'].replace(industries_df['nombre'], inplace=True)\n",
    "    users_df['id_personalidad'].replace(personalities_df['personalidad'], inplace=True)\n",
    "\n",
    "    # Se reemplazan nombres de columnas\n",
    "    users_df.rename(columns={\n",
    "        'id_industria_actual': 'industria_actual',\n",
    "        'id_industria_adicional': 'industria_adicional',\n",
    "        'id_personalidad': 'personalidad'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Se reemplazan valores nulos texto vacío según corresponda\n",
    "    users_df['postgrado'].fillna(\"\", inplace=True)\n",
    "    users_df['brief'].fillna(\"\", inplace=True)\n",
    "    users_df['industria_actual'].fillna(\"\", inplace=True)\n",
    "    users_df['industria_adicional'].fillna(\"\", inplace=True)\n",
    "    users_df['personalidad'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    # Get Communities\n",
    "    # Get column names\n",
    "    cursor.execute(\"show columns from Communities;\")\n",
    "    communities_column_names = cursor.fetchall()\n",
    "    communities_column_names = [column[0] for column in communities_column_names]\n",
    "\n",
    "    # Get communities data\n",
    "    cursor.execute(\"SELECT * FROM Communities;\")\n",
    "    output = cursor.fetchall()\n",
    "    communities_df = pd.DataFrame(output, columns=communities_column_names)\n",
    "    communities_df = communities_df.set_index('id')\n",
    "\n",
    "    # Normalize data\n",
    "    communities_df.drop([\n",
    "        'createdAt',\n",
    "        'updatedAt'\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "    communities_df\n",
    "\n",
    "    # Get useful interaction data\n",
    "\n",
    "    # Get users communities data\n",
    "    cursor.execute(\"show columns from UsuariaCommunities;\")\n",
    "    users_communities_column_names = cursor.fetchall()\n",
    "    users_communities_column_names = [column[0] for column in users_communities_column_names]\n",
    "\n",
    "    cursor.execute(f\"SELECT * FROM UsuariaCommunities WHERE userId = '{userId}';\")\n",
    "    output = cursor.fetchall()\n",
    "    users_communities_df = pd.DataFrame(output, columns=users_communities_column_names)\n",
    "    users_communities_df = users_communities_df.set_index('id')\n",
    "    users_communities_df.drop([\n",
    "        'createdAt',\n",
    "        'updatedAt'\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "    # Get posts data\n",
    "    cursor.execute(\"show columns from Posts;\")\n",
    "    posts_column_names = cursor.fetchall()\n",
    "    posts_column_names = [column[0] for column in posts_column_names]\n",
    "\n",
    "    cursor.execute(f\"SELECT * FROM Posts WHERE userId = '{userId}';\")\n",
    "    output = cursor.fetchall()\n",
    "    posts_df = pd.DataFrame(output, columns=posts_column_names)\n",
    "    posts_df = posts_df.set_index('id')\n",
    "    posts_df.drop([\n",
    "        'edited',\n",
    "        'content',\n",
    "        'createdAt',\n",
    "        'updatedAt'\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "    # Get users likes in communities data\n",
    "    cursor.execute(f\"SELECT PostLikes.id, PostLikes.usuariaId, Posts.communityId FROM PostLikes JOIN Posts ON PostLikes.postId = Posts.id WHERE Posts.userId = '{userId}';\")\n",
    "    output = cursor.fetchall()\n",
    "    users_likes_df = pd.DataFrame(output, columns=['id', 'usuariaId', 'communityId'])\n",
    "    users_likes_df = users_likes_df.set_index('id')\n",
    "    \n",
    "    # Close cursor and connection\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "    \n",
    "    return users_df, communities_df, users_communities_df, posts_df, users_likes_df\n",
    "\n",
    "def interaction(user_id, community_id, posts_df, users_likes_df):\n",
    "    # 5 puntos si pertenece a la comunidad\n",
    "    # Si se está ejecutando esta función, es porque la usuaria pertenece a la comunidad\n",
    "    points = 5\n",
    "\n",
    "    # 1 punto por cada like en la comunidad\n",
    "    points += users_likes_df[(users_likes_df.usuariaId == user_id) & (users_likes_df.communityId == community_id)].shape[0]\n",
    "\n",
    "    # 3 puntos por cada post en la comunidad\n",
    "    points += posts_df[(posts_df.communityId == community_id) & (posts_df.userId == user_id)].shape[0] * 3\n",
    "\n",
    "    return points\n",
    "\n",
    "def build_interactions_df(user_id, communities_df, users_communities_df, posts_df, users_likes_df):\n",
    "    # Set interactions dataframe\n",
    "\n",
    "    # Use communities ids as column names\n",
    "    interactions_df = pd.DataFrame(columns=communities_df.index)\n",
    "    interactions_df.index.name = 'userId'\n",
    "\n",
    "    for community_id in communities_df.index:\n",
    "        interactions_df.loc[user_id, community_id] = interaction(user_id, community_id, posts_df, users_likes_df)\n",
    "\n",
    "    # Reemplazar valores nulos por 0\n",
    "    interactions_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Normalizar los valores\n",
    "    scaler = MinMaxScaler()\n",
    "    interactions_df = pd.DataFrame(scaler.fit_transform(interactions_df.T).T, columns=interactions_df.columns, index=interactions_df.index)\n",
    "    \n",
    "    return interactions_df\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Eliminar palabras vacías (stop words)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('spanish')]  # Asumiendo que el texto está en español\n",
    "\n",
    "    # Lemmatización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Volver a unir el texto\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def user_embedding(users_df, communities_df, user_communities_df, posts_df, user_likes_df):\n",
    "    # Se junta todo el texto en una sola columna\n",
    "    users_df['text'] = users_df['postgrado'] + ' ' + users_df['industria_actual'] + ' ' + users_df['industria_adicional'] + ' ' + users_df['intereses'] + ' ' + users_df['brief'] + ' ' + users_df['personalidad']\n",
    "    users_df.drop(['postgrado', 'industria_actual', 'industria_adicional', 'intereses', 'brief', 'personalidad'], axis=1, inplace=True)\n",
    "\n",
    "    # Aplicar la función de preprocesamiento\n",
    "    users_df['text'] = users_df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Tokenizar el texto\n",
    "    users_df['tokenized'] = users_df['text'].apply(word_tokenize)\n",
    "\n",
    "    # Entrenar un modelo Word2Vec\n",
    "    model = Word2Vec(sentences=users_df['tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # O convertir todo el texto en un vector (puedes promediar los vectores de todas las palabras en el texto)\n",
    "    def document_vector(doc):\n",
    "        return np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)\n",
    "\n",
    "    users_df['vector'] = users_df['tokenized'].apply(document_vector)\n",
    "\n",
    "    # Representar el vector como un dataframe\n",
    "    user_vector_df = pd.DataFrame(users_df['vector'].to_list(), index=users_df.index)\n",
    "\n",
    "    # Normalizar los vectores\n",
    "    scaler = MinMaxScaler()\n",
    "    user_vector_df = pd.DataFrame(scaler.fit_transform(user_vector_df.T).T, columns=user_vector_df.columns, index=user_vector_df.index)\n",
    "    \n",
    "    # Obtener interacciones\n",
    "    user_id = users_df.index.values[0]\n",
    "    interactions = build_interactions_df(user_id, communities_df, user_communities_df, posts_df, user_likes_df)\n",
    "\n",
    "    return np.array([np.array(user_vector_df).tolist()[0] + np.array(interactions).tolist()[0]])\n",
    "\n",
    "def one_hot_community(communityId, nItems):\n",
    "    communityId[0] = str(int(communityId[0])-1)\n",
    "    with tf.compat.v1.Session() as tf_sess:\n",
    "        processed_community = np.array(tf_sess.run(tf.one_hot(communityId, depth=nItems)).tolist())\n",
    "    return processed_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data, communities_df, user_communities_df, posts_df, user_likes_df = get_data(userId)\n",
    "user_vector = user_embedding(user_data, communities_df, user_communities_df, posts_df, user_likes_df)\n",
    "community = one_hot_community(communityId, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vals = {\"instances\": [[user_vector[0].tolist(), community[0].tolist()]]}\n",
    "pred = predictor.predict(input_vals)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
