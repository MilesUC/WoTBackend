{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import boto3  \n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "# Set up the SageMaker session\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  \n",
    "# Delete the data directory if it already exists  \n",
    "rm -r data\n",
    "\n",
    "# Create a new directory\n",
    "mkdir data\n",
    "mkdir data/s3\n",
    "\n",
    "# Get the training data\n",
    "python3 getData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):  \n",
    "    number_of_communities = len(df.columns)\n",
    "    holdout_num = math.floor(number_of_communities * 0.2)\n",
    "    columns = ['userId', 'community', 'interaction']\n",
    "\n",
    "    # get sets with userId as index\n",
    "    df_train = pd.DataFrame(columns=columns)\n",
    "    df_test = pd.DataFrame(columns=columns)\n",
    "    for userId in df.index:\n",
    "        communities = np.random.choice(df.columns, size=holdout_num, replace=False)\n",
    "        for i in df.columns:\n",
    "            if i in communities:\n",
    "                df_test.loc[len(df_test)] = [userId, i, df.loc[userId][i]]\n",
    "            else:\n",
    "                df_train.loc[len(df_train)] = [userId, i, df.loc[userId][i]]\n",
    "      \n",
    "    return df_train, df_test  \n",
    "\n",
    "def negative_sampling(interactions_train, items, n_neg):  \n",
    "    neg = []\n",
    "    user_ids = interactions_train.userId.values\n",
    "      \n",
    "    # for every positive label case  \n",
    "    for user_id in user_ids:  \n",
    "        # generate n_neg negative labels  \n",
    "        communities_rated = interactions_train[interactions_train['userId'] == user_id]['community']\n",
    "        communities_not_rated = np.setdiff1d(items, communities_rated)\n",
    "        negative_communities = np.random.choice(communities_not_rated, size=n_neg)\n",
    "        for community in negative_communities:\n",
    "            neg.append([user_id, community, 0])\n",
    "              \n",
    "    # convert to pandas dataframe for concatenation later  \n",
    "    df_neg = pd.DataFrame(neg, columns=['userId', 'community', 'interaction'])  \n",
    "      \n",
    "    return df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de la carpet data\n",
    "interactions_df = pd.read_csv('data/interactions.csv')\n",
    "interactions_df = interactions_df.set_index('userId')\n",
    "users_vectors_df = pd.read_csv('data/users_vectors.csv')\n",
    "users_vectors_df = users_vectors_df.set_index('id')\n",
    "\n",
    "# perform train test split    \n",
    "interactions_train, interactions_test = train_test_split(interactions_df)\n",
    "\n",
    "# create 5 negative samples per positive label for training set    \n",
    "neg_train = negative_sampling(    \n",
    "    interactions_train=interactions_train,\n",
    "    items=interactions_df.columns,\n",
    "    n_neg=5\n",
    ")\n",
    "\n",
    "# create final training and testing sets\n",
    "interactions_train = interactions_train[['userId', 'community']].assign(interaction=1)\n",
    "interactions_train = pd.concat([interactions_train, neg_train], ignore_index=True)\n",
    "\n",
    "interactions_test = interactions_test[['userId', 'community']].assign(interaction=1)\n",
    "\n",
    "# save data locally first\n",
    "dest = 'data/s3'\n",
    "train_path = os.path.join(dest, 'interactions_train.npy')\n",
    "test_path = os.path.join(dest, 'interactions_test.npy')\n",
    "interactions_path = os.path.join(dest, 'interactions.csv')\n",
    "users_vectors_path = os.path.join(dest, 'users_vectors.csv')\n",
    "np.save(train_path, interactions_train.values)\n",
    "np.save(test_path, interactions_test.values)\n",
    "interactions_df.to_csv(interactions_path)\n",
    "users_vectors_df.to_csv(users_vectors_path)\n",
    "    \n",
    "# store data in the default S3 bucket  \n",
    "print(\"the default bucket name is\", bucket)  \n",
    "  \n",
    "# upload to the default s3 bucket\n",
    "sess.upload_data(train_path, key_prefix='data')\n",
    "sess.upload_data(test_path, key_prefix='data')\n",
    "sess.upload_data(interactions_path, key_prefix='data')\n",
    "sess.upload_data(users_vectors_path, key_prefix='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "device = 'gpu'\n",
    "epochs = '3'\n",
    "\n",
    "job_name = '{}-recomendation-{}-{}-{}e'.format(\n",
    "    date,\n",
    "    instance_type.replace('.','-').replace('ml-', ''),\n",
    "    device,\n",
    "    epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_estimator = TensorFlow(  \n",
    "    entry_point='ncf.py',  \n",
    "    role=role,  \n",
    "    instance_count=1,  \n",
    "    instance_type=instance_type,\n",
    "    framework_version='2.1.0',\n",
    "    py_version='py3',\n",
    "    distributions={'parameter_server': {'enabled': True}},\n",
    "    hyperparameters={'epochs': epochs, 'batch_size': 256}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_uri = os.path.join(f's3://{bucket}', 'data')  \n",
    "ncf_estimator.fit(training_data_uri, wait=True, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo SageMaker a partir del modelo entrenado\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=ncf_estimator.model_data,\n",
    "    role=role,\n",
    "    framework_version='2.1.0',\n",
    "    # image_uri='123456789012.dkr.ecr.tu-region.amazonaws.com/tu-repositorio-inferencia:tu-tag'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "instance_type = 'ml.c5.xlarge'\n",
    "endpoint_name = '{}-recomendation-model'.format(date)\n",
    "model_name = \"neural-collab-filtering-model\"\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
